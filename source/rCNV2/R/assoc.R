#!/usr/bin/env Rscript

######################
#    rCNV Project    #
######################

# Copyright (c) 2021-Present Ryan L. Collins and the Talkowski Laboratory
# Distributed under terms of the MIT License (see LICENSE)
# Contact: Ryan L. Collins <rlcollins@g.harvard.edu>

# Functions used in association testing


#' Get sample counts from table
#'
#' Extract case/control sample sizes from a single cohort for association testing
#'
#' @param pheno.table.in path to phenotype table .tsv
#' @param cohort.name name of cohort
#' @param case.hpo HPO code for cases
#' @param control.hpo HPO code for controls
#'
#' @return list of two values:
#' 1. `$case.n` : number of cases
#' 2. `$control.n` : number of controls
#'
#' @export
get.sample.counts <- function(pheno.table.in, cohort.name, case.hpo, control.hpo){
  ptab <- read.table(pheno.table.in, header=T, sep="\t", comment.char="")
  case.n <- ptab[which(ptab[,1] == case.hpo),
                 which(colnames(ptab) == cohort.name)]
  control.n <- ptab[which(ptab[,1] == control.hpo),
                    which(colnames(ptab) == cohort.name)]
  return(list("case.n"=as.numeric(case.n),
              "control.n"=as.numeric(control.n)))
}


#' Load case/control CNV counts
#'
#' Loads a file of CNV counts in cases and controls per locus
#'
#' @param bed.in path to input BED3+ file
#' @param case.col.name name of column containing case CNV counts
#' @param case.n total number of case samples in cohort
#' @param control.col.name name of column containing control CNV counts
#' @param control.n total number of control samples in cohort
#' @param keep.n.cols number of columns from original BED format to retain
#'
#' @return data frame of CNV counts and frequencies per locus
#'
#' @details `bed.in` must adhere to format as generated by any of the following scripts:
#' * `analysis/sliding_windows/count_cnvs_per_window.py`
#' * `analysis/genes/count_cnvs_per_gene.py`
#' * `analysis/noncoding/count_cnvs_per_crb.py`
#'
#' @export
load.cc.cnv.counts <- function(bed.in, case.col.name, case.n, control.col.name,
                               control.n, keep.n.cols=3){
  bed <- read.table(bed.in, sep="\t", header=T, comment.char="")

  case.col.idx <- which(colnames(bed) == case.col.name)
  if(length(case.col.idx) == 0){
    stop(paste("--case-column \"",case.col.name,"\" cannot be found ",
               "in BED header", sep=""))
  }

  control.col.idx <- which(colnames(bed) == control.col.name)
  if(length(control.col.idx) == 0){
    stop(paste("--control-column \"",control.col.idx,"\" cannot be found ",
               "in BED header", sep=""))
  }

  bed <- bed[,c(1:keep.n.cols, case.col.idx, control.col.idx)]
  colnames(bed) <- c("chr", colnames(bed[2:keep.n.cols]), "case.CNV", "control.CNV")
  bed$case.ref <- case.n - bed$case.CNV
  bed$control.ref <- control.n - bed$control.CNV
  bed$case.CNV.freq <- bed$case.CNV / case.n
  bed$control.CNV.freq <- bed$control.CNV / control.n

  return(bed)
}


#' Single-locus Fisher's exact test
#'
#' Fisher's exact test for a single vector of CNV counts for one cohort and one locus
#'
#' @param counts vector of CNV counts (see `Details`)
#' @param alternative specify sidedness of test \[default: 'greater'\]
#'
#' @details `counts` must contain four values, in order:
#' 1. count of case CNV carriers
#' 2. count of control CNV carriers
#' 3. count of non-carrier cases
#' 4. count of non-carrier controls
#'
#'
#' @return named vector with association summary statistics, in the order of
#' P-value, odds ratio, and lower/upper bounds of 95% conf. int. of the OR
#'
#' @export
fisher.burden.test.single <- function(counts, alternative="greater"){
  case.cnv <- as.integer(counts[1])
  control.cnv <- as.integer(counts[2])
  case.ref <- as.integer(counts[3])
  control.ref <- as.integer(counts[4])

  if(case.cnv == 0 & control.cnv == 0){
    p <- 1
    or <- c(NA, NA, NA)
  }else{
    cnv.mat <- matrix(c(control.ref, case.ref, control.cnv, case.cnv),
                      byrow=T, nrow=2)

    p <- fisher.test(cnv.mat, alternative=alternative)$p.value
    or <- calc.or(control.ref, control.cnv, case.ref, case.cnv, return.confint=T)
  }

  f.res <- as.numeric(c(p, or))
  names(f.res) <- c("p", "OR", "OR.lower", "OR.upper")
  return(f.res)
}


#' Compute Fisher's exact test lookup table
#'
#' Build a lookup table of all case/control count pairings for Fisher's Exact Tests
#'
#' @param bed data frame of case/control CNV counts as loaded by `load.cc.cnv.counts()`
#' @param keep.n.cols number of columns from original BED format to retain
#'
#' @return data frame of all unique case & control CNV counts
#'
#' @seealso [load.cc.cnv.counts()]
#'
#' @export
build.fisher.lookup.table <- function(bed, keep.n.cols=3){
  counts.df <- unique(bed[, (1:4)+keep.n.cols])
  counts.df <- counts.df[with(counts.df, order(control.CNV, case.CNV)),]

  f.stats <- t(apply(counts.df, 1, fisher.burden.test.single))

  f.table <- cbind(counts.df, f.stats)
  return(f.table)
}


#' Fisher's Exact Test for a list of loci
#'
#' Fisher's exact test of case:control CNV burden per locus for one or more loci
#'
#' @param bed data frame of case/control CNV counts as loaded by `load.cc.cnv.counts()`
#' @param keep.n.cols number of columns from original BED format to retain
#' @param precision maximum precision for floating point values
#'
#' @return data frame of association test resuts
#'
#' @seealso [fisher.burden.test.single()]
#'
#' @export
fisher.burden.test <- function(bed, keep.n.cols=3, precision=10){
  f.table <- build.fisher.lookup.table(bed, keep.n.cols)

  f.res <- merge(bed, f.table, sort=F, all.x=T, all.y=F)
  f.res <- f.res[with(f.res, order(chr, start)),]

  fisher.bed <- data.frame("chr" = f.res$chr,
                           "start" = f.res$start,
                           "end" = f.res$end,
                           "case_alt" = f.res$case.CNV,
                           "case_ref" = f.res$case.ref,
                           "case_freq" = round(f.res$case.CNV.freq, precision),
                           "control_alt" = f.res$control.CNV,
                           "control_ref" = f.res$control.ref,
                           "control_freq" = round(f.res$control.CNV.freq, precision),
                           "fisher_neg_log10_p" = round(-log10(f.res$p), precision),
                           "fisher_OR" = round(f.res$OR, precision),
                           "fisher_OR_lower" = round(f.res$OR.lower, precision),
                           "fisher_OR_upper" = round(f.res$OR.upper, precision))

  # Merge & deduplicate (necessary for situations where two genes have different
  # symbols but identical coordinates)
  bed.slice <- bed[, unique(c(colnames(bed)[1:keep.n.cols],
                              "case.CNV", "case.ref", "control.CNV", "control.ref"))]
  colnames(bed.slice)[keep.n.cols + 1:4] <- colnames(fisher.bed)[c(4:5, 7:8)]
  out.df <- merge(bed.slice, fisher.bed, all.y=T, sort=F, by=colnames(fisher.bed)[c(1:5, 7:8)])
  first.cols <- colnames(bed)[1:keep.n.cols]
  latter.cols <- colnames(fisher.bed)[which(!colnames(fisher.bed) %in% first.cols)]
  return(out.df[!duplicated(out.df), c(first.cols, latter.cols)])
}


#' Calculate CNV odds ratio
#'
#' Compute basic OR of CNV carriers with adjustment for zero-inflation
#'
#' @param control_ref number of control non-carriers
#' @param control_alt number of control CNV carriers
#' @param case_ref number of case non-carriers
#' @param case_alt number of case CNV carriers
#' @param adj fixed value to use for continuity correction \[default: 0.5\]
#' @param return.confint return confidence interval \[default: false\]
#' @param alpha alpha value corresponding to `return.confint` \[default: 0.05\]
#'
#' @details implements simple continuity correction with fixed value added to
#' all cells of contingency table
#'
#' @return numeric vector of odds ratio, optionally with lower and upper 95% confidence bounds
#'
#' @export
calc.or <- function(control_ref, control_alt, case_ref, case_alt, adj=0.5,
                    return.confint=FALSE, alpha=0.05){
  case.odds <- (case_alt + adj) / (case_ref + adj)
  control.odds <- (control_alt + adj) / (control_ref + adj)
  or <- case.odds / control.odds
  if(return.confint){
    z <- qnorm(1 - (alpha / 2))
    sd <- sqrt(sum(1 / (c(control_ref, control_alt, case_ref, case_alt) + adj)))
    ci <- exp(log(or) + (c(-1, 1) * z * sd))
    return(c(or, ci))
  }else{
    return(or)
  }
}


#' Multi-cohort pairwise effect size comparison
#'
#' Make a grid of scatterplots of log odds ratios pairwise for all cohorts in a meta-analysis
#'
#' @param stats.list list of single-cohort association stats
#' @param pt.cex scaling factor for points
#'
#' @return None
#'
#' @seealso [read.assoc.stats.single()]
#'
#' @export or.corplot.grid
#' @export
or.corplot.grid <- function(stats.list, pt.cex=1){
  ncohorts <- length(stats.list)
  cohorts <- names(stats.list)
  par(mfrow=c(ncohorts, ncohorts))
  ymar <- 3
  xmar <- 3.7
  sapply(1:ncohorts, function(r){
    sapply(1:ncohorts, function(c){
      parmar <- c(ymar/2, xmar/2, ymar/2, xmar/2)
      # Set margins
      if(c==1){
        parmar[2] <- xmar-0.2; parmar[4] <- 0.2
      }
      if(c==ncohorts){
        parmar[2] <- 0.2; parmar[4] <- xmar-0.2
      }
      if(r==1){
        parmar[1] <- 0.2; parmar[3] <- ymar-0.2
      }
      if(r==ncohorts){
        parmar[1] <- ymar-0.2; parmar[3] <- 0.2
      }
      # Don't plot self-self correlations
      if(c==r){
        par(mar=parmar)
        plot(x=c(0, 1), y=c(0, 1), type="n",
             xaxt="n", xlab="", yaxt="n", ylab="")
        rect(xleft=par("usr")[1], xright=par("usr")[2],
             ybottom=par("usr")[3], ytop=par("usr")[4],
             bty="n", col="gray95")
        text(x=0.5, y=0.5, labels=bquote(italic(R)==1))
        # Otherwise, plot as normal
      }else{
        dens.scatter(x=log10(stats.list[[c]][, grep("OR", colnames(stats.list[[c]]), fixed=T)]),
                     y=log10(stats.list[[r]][, grep("OR", colnames(stats.list[[r]]), fixed=T)]),
                     parmar=parmar, pt.cex=pt.cex)
      }
      # Add headers & axes
      if(c==1){
        mtext(2, line=0.2, text=cohorts[r], font=2)
      }
      if(r==1){
        mtext(3, line=0.2, text=cohorts[c], font=2)
      }
      if(c==ncohorts){
        axis(4, labels=NA)
        axis(4, tick=F, line=-0.5, las=2)
        mtext(4, line=2.25, text=bquote(log[10](OR)), cex=0.8)
      }
      if(r==ncohorts){
        axis(1, labels=NA)
        axis(1, tick=F, line=-0.5)
        mtext(1, line=2, text=bquote(log[10](OR)) ,cex=0.8)
      }
    })
  })
}


#' Merge single-cohort association stats
#'
#' Merges a list of association statistics generated for multiple independent cohorts
#'
#' @param stats.list list of single-cohort association stats
#' @param cond.excl.in path to BED file of cohorts to be excluded on locus-specific basis
#' @param min.cases minimum number of cases required to include a cohort in meta-analysis \[default: 1\]
#' @param keep.n.cols number of columns from original BED format to retain \[default: 3\]
#' @param min.case.lookahead number of head rows to consider when evaluating `min.cases` criteria \[default: 20\]
#'
#' @return data frame of association stats for all cohorts
#'
#' @seealso [read.assoc.stats.single()]
#'
#' @export
combine.single.cohort.assoc.stats <- function(stats.list, cond.excl.in=NULL,
                                              min.cases=1, keep.n.cols=3,
                                              min.case.lookahead=20){
  # Merge all cohorts
  merged <- stats.list[[1]]
  mergeby.cols <- colnames(merged)[1:keep.n.cols]
  for(i in 2:length(stats.list)){
    merged <- merge(merged, stats.list[[i]], by=mergeby.cols, all=F, sort=F)
  }
  merged[, -c(1:keep.n.cols)] <- apply(merged[, -c(1:keep.n.cols)], 2, as.numeric)

  # Count number of nominally significant individual cohorts
  n_nom_sig <- apply(merged[, grep(".p_value", colnames(merged), fixed=T)],
                     1, function(pvals){length(which(pvals<=0.05))})
  merged$n_nominal_cohorts <- n_nom_sig

  # Determine most significant cohort per row
  merged$top_cohort <- unlist(apply(merged[, grep(".p_value", colnames(merged), fixed=T)], 1,
                                    function(pvals){
                                      head(names(stats.list)[which(pvals == min(pvals, na.rm=T))], 1)
                                    }))
  merged <- merged[, -grep(".p_value", colnames(merged), fixed=T)]

  # Annotate rows with cohorts to be conditionally excluded
  if(!is.null(cond.excl.in)){
    cond.excl.df <- read.table(cond.excl.in, header=T, sep="\t", comment.char="")
    colnames(cond.excl.df)[1:keep.n.cols] <- mergeby.cols[1:keep.n.cols]
    merged <- merge(merged, cond.excl.df, by=mergeby.cols,
                    all.x=T, all.y=F, sort=F)
  }else{
    merged$exclude_cohorts <- ""
  }

  # Identify cohorts below min.cases threshold to be excluded
  low.caseCount.cohorts <- unlist(sapply(names(stats.list), function(cohort){
    df <- stats.list[[cohort]]
    n.cases <- max(apply(head(df[, grep(".case_", colnames(df), fixed=T)],
                              min.case.lookahead), 1, sum, na.rm=T), na.rm=T)
    if(n.cases < min.cases){
      return(cohort)
    }
  }))
  if(!is.null(low.caseCount.cohorts)){
    merged$exclude_cohorts <- as.character(sapply(merged$exclude_cohorts, function(xstr){
      paste(sort(unique(c(unlist(strsplit(xstr, split=";")), low.caseCount.cohorts))), collapse=";")
    }))
  }

  return(as.data.frame(merged))
}


#' Standardize effect sizes
#'
#' Standardize effect sizes generated by single-cohort association tests
#'
#' @param or point estimate of odds ratio
#' @param ci.lower lower bound of odds ratio confidence interval
#' @param ci.upper upper bound of odds ratio confidence interval
#' @param alpha alpha value corresponding to confidence interval \[default: 0.05\]
#'
#' @return numeric vector of standardized effect sizes reported as log-odds ratio
#' divided by standard deviation
#'
#' @export standardize.effect.size
#' @export
standardize.effect.size <- function(or, ci.lower, ci.upper, alpha=0.05){
  sd <- (log(ci.upper) - log(or)) / qnorm(1 - (alpha / 2))
  log(or) / sd
}


#' Estimate cohort inflation terms
#'
#' Estimates mean genome-wide standardized effect size per cohort
#'
#' @param stats.df association statistics for a single cohorts as read by [read.assoc.stats.single()]
#'
#' @return named vector with mean standardized log odds ratio for each cohort
#'
#' @details Note that `keep.or.confint` must be `TRUE` when reading data
#' with [`read.assoc.stats.single()`] for this method to work
#'
#' @seealso [combine.single.cohort.assoc.stats()]
#'
#' @export estimate.cohort.inflation
#' @export
estimate.cohort.inflation <- function(stats.df){
  # Compute standardized effect sizes for all windows
  or.df <- stats.df[, grep("fisher_OR", colnames(stats.df), fixed=T)]
  ses <- apply(or.df, 1, function(v){standardize.effect.size(v[1], v[2], v[3])})
  mean(ses, na.rm=T)
}


#' Empirical continuity correction for meta-analyses
#'
#' Apply empirical continuity correction to meta-analysis data frame per Sweeting
#' et al., Stat. Med., 2004 (section 3.3)
#'
#' @param meta.df meta-analysis data frame as generated by [make.meta.df()]
#' @param cc.sum scaling factor for total continuity correction to be distributed
#' among cases and controls
#'
#' @return data frame of meta-analysis stats after applying continuity correction
#'
#' @seealso [make.meta.df()]
#'
#' @export
sweeting.correction <- function(meta.df, cc.sum=0.01){
  # Count number of carriers & non-carriers
  n.alt <- sum(meta.df[, grep("_alt", colnames(meta.df), fixed=T)])
  n.ref <- sum(meta.df[, grep("_ref", colnames(meta.df), fixed=T)])
  # Require at least one CNV to be observed
  if(n.alt>0){
    nt <- n.alt
    R <- n.ref/n.alt
    # Pooled odds ratio estimate of all non-zero studies with at least one case sample
    nonzero.studies <- intersect(which(apply(meta.df[, grep("_alt", colnames(meta.df), fixed=T)], 1, sum)>0),
                                 which(apply(meta.df[, grep("case_", colnames(meta.df), fixed=T)], 1, sum)>0))
    nonzero.case.odds <- sum(meta.df$case_alt[nonzero.studies])/sum(meta.df$case_ref[nonzero.studies])
    nonzero.control.odds <- sum(meta.df$control_alt[nonzero.studies])/sum(meta.df$control_ref[nonzero.studies])
    if(!is.nan(nonzero.case.odds) & !is.nan(nonzero.control.odds) &
       !is.infinite(nonzero.case.odds) & !is.infinite(nonzero.control.odds)){
      if(nonzero.control.odds>0){
        ohat <- nonzero.case.odds/nonzero.control.odds
        # Otherwise, apply standard continuity correction of 0.5 to pooled estimate if no CNVs observed in controls or if either case/control odds is infinite
      }else{
        nonzero.case.odds <- (sum(meta.df$case_alt[nonzero.studies])+0.5)/(sum(meta.df$case_ref[nonzero.studies])+0.5)
        nonzero.control.odds <- (sum(meta.df$control_alt[nonzero.studies])+0.5)/(sum(meta.df$control_ref[nonzero.studies])+0.5)
        ohat <- nonzero.case.odds/nonzero.control.odds
      }
      # Otherwise, apply standard continuity correction to pooled estimate of *all* studies
    }else{
      nonzero.case.odds <- (sum(meta.df$case_alt)+0.5)/(sum(meta.df$case_ref)+0.5)
      nonzero.control.odds <- (sum(meta.df$control_alt)+0.5)/(sum(meta.df$control_ref)+0.5)
      ohat <- nonzero.case.odds/nonzero.control.odds
    }

    # Solve for kc & kt
    kc <- R/(R+ohat)
    kt <- ohat/(R+ohat)
    # Compute continuity corrections
    cor.case_alt <- cc.sum * kt
    cor.case_ref <- cc.sum * kc
    cor.control_alt <- cc.sum * (nt + kt)
    cor.control_ref <- cc.sum * ((nt*R) + kc)
    # Apply continuity corrections
    meta.df$case_alt <- meta.df$case_alt + cor.case_alt
    meta.df$case_ref <- meta.df$case_ref + cor.case_ref
    meta.df$control_alt <- meta.df$control_alt + cor.control_alt
    meta.df$control_ref <- meta.df$control_ref + cor.control_ref
  }
  return(meta.df)
}


#' Create meta-analysis data frame
#'
#' Make meta-analysis data frame for CNV association testing of a single locus
#'
#' @param stats.merged association statistics for all cohorts (see [combine.single.cohort.assoc.stats()])
#' @param cohorts vector of all cohort names
#' @param row.idx row index in `stats.merged` corresponding to locus of interest
#' @param empirical.continuity boolean indicator to apply Sweeting empirical
#' continuity correction \[default: TRUE\]
#'
#' @return data frame of statistics ready for meta-analysis
#'
#' @seealso [combine.single.cohort.assoc.stats()]
#'
#' @export
make.meta.df <- function(stats.merged, cohorts, row.idx, empirical.continuity=T){
  ncohorts <- length(cohorts)
  meta.df <- data.frame("cohort"=1:ncohorts,
                        "control_ref"=as.numeric(stats.merged[row.idx, grep("control_ref", colnames(stats.merged), fixed=T)]),
                        "case_ref"=as.numeric(stats.merged[row.idx, grep("case_ref", colnames(stats.merged), fixed=T)]),
                        "control_alt"=as.numeric(stats.merged[row.idx, grep("control_alt", colnames(stats.merged), fixed=T)]),
                        "case_alt"=as.numeric(stats.merged[row.idx, grep("case_alt", colnames(stats.merged), fixed=T)]),
                        "cohort_name"=cohorts)
  if(empirical.continuity==T){
    meta.df <- sweeting.correction(meta.df)
  }
  return(meta.df)
}


#' Single locus meta-analysis
#'
#' Perform a meta-analysis of CNVs for a single locus
#'
#' @param stats.merged association statistics for all cohorts (see [combine.single.cohort.assoc.stats()])
#' @param cohorts vector of all cohort names
#' @param row.idx row index in `stats.merged` corresponding to locus of interest
#' @param model specify meta-analysis model to use (see `Details`)
#' @param adjust.biobanks boolean indicator to include biobank status as a categorical
#' covariate in meta-analysis model \[default: FALSE\]
#' @param cohort.inflation numeric vector of inflation terms for each cohort as
#' estimated by [estimate.cohort.inflation()] \[default: NULL]
#' @param probe.counts data frame of control probe counts (see `Details`) \[default: NULL\]
#' @param empirical.continuity boolean indicator to apply Sweeting empirical
#' continuity correction \[default: TRUE\]
#' @param drop_top_cohort boolean indicator to drop most significant individual
#' cohort from meta-analysis \[default: FALSE\]
#'
#' @details `model` accepts several character inputs, including:
#' * `fe` : fixed-effects model implemented by [metafor::rma.uni()] \[default\]
#' * `re` : random-effects model implemented by [metafor::rma.uni()]
#' * `mh` : Mantel-Haenszel model implemented by [metafor::rma.mh()]
#'
#' `probe.counts` must be a BED-style data frame matching the exact entries in
#' stats.merged with one numeric column of probe counts for each cohort.
#'
#' @return numeric vector of meta-analysis summary statistics
#'
#' @seealso [make.meta.df()]
#'
#' @export
meta.single <- function(stats.merged, cohorts, row.idx, model="fe",
                        adjust.biobanks=F, cohort.inflation=NULL,
                        probe.counts=NULL, empirical.continuity=T,
                        drop_top_cohort=F){
  require(metafor, quietly=T)
  # Collect list of cohorts to exclude
  exclude.cohorts <- unlist(strsplit(stats.merged$exclude_cohorts[row.idx], split=";"))
  if(drop_top_cohort == TRUE){
    exclude.cohorts <- unique(sort(c(exclude.cohorts, stats.merged$top_cohort[row.idx])))
  }
  exclude.cohorts <- exclude.cohorts[which(exclude.cohorts %in% cohorts)]
  cohorts <- setdiff(cohorts, exclude.cohorts)

  # Hard drop cohorts for exclusion from stats.merged for this row
  if(length(exclude.cohorts) > 0){
    cols.to.drop <- unique(sort(as.vector(sapply(exclude.cohorts, function(cohort){
      grep(paste(cohort, ".", sep=""), colnames(stats.merged))
    }))))
    if(length(cols.to.drop) > 0){
      stats.merged.sub <- stats.merged[row.idx, -cols.to.drop]
    }
  }else{
    stats.merged.sub <- stats.merged[row.idx, ]
  }

  # Check that at least two cohorts remain
  n.cohorts <- length(cohorts)
  if(n.cohorts >= 2){
    # Check if at least one CNV was observed
    n.cnvs <- sum(stats.merged.sub[, grep("_alt", colnames(stats.merged.sub), fixed=T)])
    if(n.cnvs > 0){
      meta.df <- make.meta.df(stats.merged.sub, cohorts, 1, empirical.continuity)
      # Disable biobank correction term if there aren't at least two biobanks & two non-biobanks
      if(length(which(cohorts %in% biobanks)) < 2 | length(which(!(cohorts %in% biobanks))) < 2){
        adjust.biobanks <- FALSE
      }
      # Add covariates, if optioned
      if(adjust.biobanks | !is.null(cohort.inflation) | !is.null(probe.counts)){
        mods <- c()
        if(adjust.biobanks){
          meta.df$is_biobank <- as.numeric(meta.df$cohort_name %in% biobanks)
          mods <- c(mods, "is_biobank")
        }
        if(!is.null(cohort.inflation)){
          meta.df$inflation <- cohort.inflation[meta.df$cohort_name]
          mods <- c(mods, "inflation")
        }
        if(!is.null(probe.counts)){
          meta.df$probe_count <- as.numeric(probe.counts[row.idx, meta.df$cohort_name])
          mods <- c(mods, "probe_count")
        }
        mods <- as.formula(paste("~", paste(mods, collapse=" + ")))
      }else{
        mods <- NULL
      }

      # If strictly zero case CNVs are observed, unable to estimate effect size
      if(nrow(meta.df) > 0){
        if(all(meta.df$case_alt==0)){
          out.v <- c(rep(NA, 4), 0)
        }else{
          # Meta-analysis
          if(model=="re"){
            meta.res <- tryCatch(rma.uni(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt, mods=mods,
                                         measure="OR", data=meta.df, method="REML", random = ~ 1 | cohort, slab=cohort_name,
                                         add=0, drop00=F, correct=F, digits=5, control=list(maxiter=100, stepadj=0.5)),
                                 error=function(e){
                                   print(paste("row", row.idx, "failed to converge. Retrying with more iterations...", sep=" "))
                                   rma.uni(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt,
                                           measure="OR", data=meta.df, method="REML", random = ~ 1 | cohort, slab=cohort_name,
                                           add=0, drop00=F, correct=F, digits=5, control=list(maxiter=10000, stepadj=0.4))
                                 })
            out.v <- as.numeric(c(meta.res$b[1,1], meta.res$ci.lb, meta.res$ci.ub,
                                  meta.res$zval, -log10(meta.res$pval)))
          }else if(model=="mh"){
            meta.res <- rma.mh(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt, mods=mods,
                               measure="OR", data=meta.df, slab=cohort_name,
                               add=0, drop00=F, correct=F)
            out.v <- as.numeric(c(meta.res$b, meta.res$ci.lb, meta.res$ci.ub,
                                  meta.res$zval, -log10(meta.res$MHp)))
          }else if(model=="fe"){
            meta.res <- tryCatch(rma.uni(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt, mods=mods,
                                         measure="OR", data=meta.df, method="FE", slab=cohort_name,
                                         add=0, drop00=F, correct=F, digits=5, control=list(maxiter=100, stepadj=0.5)),
                                 error=function(e){
                                   print(paste("row", row.idx, "failed to converge. Retrying with more iterations...", sep=" "))
                                   rma.uni(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt,
                                           measure="OR", data=meta.df, method="FE", slab=cohort_name,
                                           add=0, drop00=F, correct=F, digits=5, control=list(maxiter=10000, stepadj=0.4))
                                 })
            out.v <- as.numeric(c(meta.res$b[1,1], meta.res$ci.lb[1], meta.res$ci.ub[1],
                                  meta.res$zval[1], -log10(meta.res$pval)[1]))
          }
          # Force to p-values reflecting Ha : OR > 1
          if(!is.na(out.v[1]) & !is.na(out.v[5])){
            if(out.v[1] < 0){
              out.v[5] <- 0
            }
          }
        }
      }
      return(out.v)
    }else{
      return(rep(NA, 5))
    }
  }else{
    return(rep(NA, 5))
  }
}


#' Create meta-analysis lookup table
#'
#' Make meta-analysis lookup table to shorten time required to run full meta-analysis
#'
#' @param stats.merged association statistics for all cohorts (see [combine.single.cohort.assoc.stats()])
#' @param cohorts vector of all cohort names
#' @param model specify meta-analysis model to use (see `Details`)
#' @param adjust.biobanks boolean indicator to include biobank status as a categorical
#' covariate in meta-analysis model \[default: FALSE\]
#' @param cohort.inflation numeric vector of inflation terms for each cohort as
#' estimated by [estimate.cohort.inflation()] \[default: NULL]
#' @param probe.counts data frame of control probe counts (see `Details`) \[default: NULL\]
#' @param empirical.continuity boolean indicator to apply Sweeting empirical
#' continuity correction \[default: TRUE\]
#'
#' @details `model` accepts several character inputs, including:
#' * `fe` : fixed-effects model implemented by [metafor::rma.uni()] \[default\]
#' * `re` : random-effects model implemented by [metafor::rma.uni()]
#' * `mh` : Mantel-Haenszel model implemented by [metafor::rma.mh()]
#'
#' `probe.counts` must be a BED-style data frame matching the exact entries in
#' stats.merged with one numeric column of probe counts for each cohort.
#'
#' @return data frame of all unique case & control CNV counts
#'
#' @export
make.meta.lookup.table <- function(stats.merged, cohorts, model, adjust.biobanks=F,
                                   cohort.inflation=NULL, probe.counts=NULL,
                                   empirical.continuity=T){
  # Make dataframe of CNV counts sorted by cohort
  counts.df <- stats.merged[, sort(unique(c(grep("_ref", colnames(stats.merged), fixed=T),
                                            grep("_alt", colnames(stats.merged), fixed=T),
                                            which(colnames(stats.merged) == "exclude_cohorts"))))]

  # If probe adjustment is not optioned, can collapse to unique counts to save runtime
  if(is.null(probe.counts)){
    counts.df <- unique(counts.df)
  }

  # Compute meta-analysis stats for all rows in counts.df
  meta.stats <- t(sapply(1:nrow(counts.df), function(i){
    meta.single(counts.df, cohorts, i, model, adjust.biobanks, cohort.inflation,
                probe.counts, empirical.continuity)
  }))

  # Format output table
  lookup.table <- cbind(counts.df, meta.stats)
  stat.colnames <- c("meta_lnOR", "meta_lnOR_lower", "meta_lnOR_upper", "meta_z", "meta_neg_log10_p")
  colnames(lookup.table)[(ncol(lookup.table)-4):ncol(lookup.table)] <- stat.colnames

  return(lookup.table)
}


#' Z-score saddlepoint approximation
#'
#' Apply saddlepoint approximation to vector of Z-scores to generate adjusted P-values
#'
#' @param zscores numeric vector of Z-scores
#' @param xidxs optional list of `zscores` indexes to exclude when fitting
#' saddlepoint \[default: include all regions \]
#' @param winsorize width of quantile intervals to Winsorize
#' \[default: 1, i.e., no Winsorization\]
#' @param winsorize.left.tail boolean indicator to apply Winsorization to left
#' tail of distribution \[default: FALSE\]
#' @param mirror mirror bottom 50% of Z-scores \[default: FALSE\]
#' @param neglog10 boolean indicator of whether to -log10-scale adjusted P-values
#'
#' @return data.frame with two columns:
#' * `$zscores` for corrected Z-scores
#' * `$pvalues` for P-values corresponding to corrected Z-scores
#'
#' @export
saddlepoint.adj <- function(zscores, xidxs=NULL, winsorize=1, winsorize.left.tail=F,
                            mirror=F, neglog10=T){
  zscores.orig <- zscores
  if(!is.null(xidxs)){
    zscores <- zscores[-xidxs]
  }
  winsor.bounds <- quantile(zscores, probs=c(1-winsorize, winsorize), na.rm=T)
  if(winsorize.left.tail){
    zscores[which(zscores < winsor.bounds[1])] <- winsor.bounds[1]
  }
  zscores[which(zscores > winsor.bounds[2])] <- winsor.bounds[2]
  if(mirror){
    z.med <- median(zscores, na.rm=T)
    bottomhalf <- zscores[which(zscores <= z.med)]
    zscores <- c(bottomhalf, abs(bottomhalf - z.med) + z.med)
  }
  mu.hat <- mean(zscores, na.rm=T)
  sd.hat <- sd(zscores, na.rm=T)
  cumuls <- gaussianCumulants(mu.hat, sd.hat)
  dx <- 0.01
  x <- seq(-40, 40, dx)
  saddle.pdf <- saddlepoint(x, 1, cumuls)$approx
  # Dev note: must infer parameters of saddlepoint-approximated normal for precise extreme P-values with pnorm()
  mu.saddle <- sum(x * saddle.pdf) * dx
  sd.saddle <- sqrt(sum(saddle.pdf * dx * (x - mu.saddle)^2))
  # Compute new Z-scores and P-values
  new.zscores <- (zscores.orig - mu.saddle) / sd.saddle
  if(neglog10==T){
    new.pvals <- -pnorm(new.zscores, lower.tail=FALSE, log.p=TRUE)/log(10)
  }else{
    new.pvals <- pnorm(new.zscores, lower.tail=FALSE)
  }
  return(data.frame("zscores" = new.zscores, "pvalues" = new.pvals))
}


#' CNV meta-analysis for one or more loci
#'
#' Conduct CNV association meta-analyses for one or more cohorts at one or more loci
#'
#' @param stats.merged association statistics for all cohorts (see [combine.single.cohort.assoc.stats()])
#' @param cohorts vector of all cohort names
#' @param model specify meta-analysis model to use (see `Details`)
#' @param saddle boolean indicator of whether to apply saddlepoint approximation
#' (see [saddlepoint.adj()]) \[default: TRUE\]
#' @param adjust.biobanks boolean indicator to include biobank status as a categorical
#' covariate in meta-analysis model \[default: FALSE\]
#' @param cohort.inflation numeric vector of inflation terms for each cohort as
#' estimated by [estimate.cohort.inflation()] \[default: NULL]
#' @param probe.counts data frame of control probe counts (see `Details`) \[default: NULL\]
#' @param saddle.exclusion BED-style dataframe of regions to exclude when fitting
#' saddlepoint approximation \[default: include all regions\]
#' @param winsorize Winzorization interval for saddlepoint adjustment
#' (see [saddlepoint.adj()]) \[default: 1, i.e., no Winsorization\]
#' @param mirror.saddle mirror bottom 50% of Z-scores prior to saddlepoint
#' approximation (see [saddlepoint.adj()]) \[default: FALSE\]
#' @param calc.fdr boolean indicator to calculate B-H FDR q-value per locus \[default: TRUE\]
#' @param secondary boolean indicator to also compute meta-analysis statistics
#' after dropping most significant individual cohort \[default: TRUE\]
#' @param keep.n.cols number of columns from original BED format to retain
#'
#' @details `model` accepts several character inputs, including:
#' * `fe` : fixed-effects model implemented by [metafor::rma.uni()] \[default\]
#' * `re` : random-effects model implemented by [metafor::rma.uni()]
#' * `mh` : Mantel-Haenszel model implemented by [metafor::rma.mh()]
#'
#' `probe.counts` must be a BED-style data frame matching the exact entries in
#' stats.merged with one numeric column of probe counts for each cohort.
#'
#' @return data frame of meta-analysis summary statistics
#'
#' @export
meta <- function(stats.merged, cohorts, model="fe", saddle=T, adjust.biobanks=F,
                 cohort.inflation=NULL, probe.counts=NULL, saddle.exclusion=NULL,
                 winsorize=1, mirror.saddle=F, calc.fdr=T, secondary=T, keep.n.cols=3){
  # Make meta-analysis lookup table
  meta.lookup.table <- make.meta.lookup.table(stats.merged, cohorts, model,
                                              adjust.biobanks, cohort.inflation,
                                              probe.counts, empirical.continuity=T)

  # Merge stats into full list
  if(is.null(probe.counts)){
    # Must be merged when probe.counts is not provided because make.meta.lookup.table()
    # only computes unique count vectors to save time
    meta.res <- merge(stats.merged, meta.lookup.table, sort=F, all.x=T, all.y=F)
  }else{
    # Otherwise, every row in stats.merged will be evaluated and can be directly
    # combined with cbind()
    meta.res <- cbind(stats.merged, meta.lookup.table[, (-4:0)+ncol(meta.lookup.table)])
  }
  meta.res <- meta.res[with(meta.res, order(chr, start)), ]

  # Adjust P-values using saddlepoint approximation of null distribution, if optioned
  if(saddle==T){
    # Find indexes to exclude during saddlepoint
    if(!is.null(saddle.exclusion)){
      spa.xidxs <- which(meta.res$chr == saddle.exclusion$chr
                         & meta.res$start <= saddle.exclusion$end
                         & meta.res$end >= saddle.exclusion$start)
      if(length(spa.xidxs) == 0){
        spa.xidxs <- NULL
      }
    }else{
      spa.xidxs <- NULL
    }
    # Apply saddlepoint adjustment
    meta.res[, c("meta_z", "meta_neg_log10_p")] <- saddlepoint.adj(meta.res$meta_z,
                                                               xidxs=spa.xidxs,
                                                               winsorize=winsorize,
                                                               mirror=mirror.saddle)
  }

  # Calculate B-H adjusted q-values, if optioned
  if(calc.fdr==T){
    meta.res$meta_neg_log10_fdr_q <- -log10(p.adjust(10^-meta.res$meta_neg_log10_p, method="BH"))
  }

  # Compute secondary P-value
  if(secondary==T){
    meta.res.secondary <- as.data.frame(t(sapply(1:nrow(meta.res), function(i){
      meta.single(meta.res, cohorts, i, model, adjust.biobanks, cohort.inflation,
                  probe.counts, empirical.continuity=T, drop_top_cohort=T)
    })))
    colnames(meta.res.secondary) <- c("meta_lnOR", "meta_lnOR_lower", "meta_lnOR_upper", "meta_z", "meta_neg_log10_p")

    # Saddlepoint on secondary, if optioned
    if(saddle==T){
      meta.res.secondary[, c("meta_z", "meta_neg_log10_p")] <- saddlepoint.adj(meta.res.secondary$meta_z,
                                                                           xidxs=spa.xidxs,
                                                                           winsorize=winsorize,
                                                                           mirror=mirror.saddle)
    }

    meta.res$meta_lnOR_secondary <- meta.res.secondary$meta_lnOR
    meta.res$meta_lnOR_lower_secondary <- meta.res.secondary$meta_lnOR_lower
    meta.res$meta_lnOR_upper_secondary <- meta.res.secondary$meta_lnOR_upper
    meta.res$meta_z_secondary <- meta.res.secondary$meta_z
    meta.res$meta_neg_log10_p_secondary <- meta.res.secondary$meta_neg_log10_p
    if(calc.fdr==T){
      meta.res$meta_neg_log10_fdr_q_secondary <- -log10(p.adjust(10^-meta.res$meta_neg_log10_p_secondary, method="BH"))
    }
  }

  # Compute pooled carrier frequencies (while taking into account excluded cohorts)
  sum_cols_helper <- function(meta.res, substring){
    hits <- grep(substring, colnames(meta.res), fixed=T)
    excl.cohorts <- strsplit(meta.res$exclude_cohorts, split=";")
    sapply(1:nrow(meta.res), function(i){
      excl <- unique(unlist(sapply(excl.cohorts[[i]], grep, x=colnames(meta.res), fixed=TRUE)))
      keep <- setdiff(hits, excl)
      sum(as.numeric(meta.res[i, keep]), na.rm=T)
    })
  }
  case.alt <- sum_cols_helper(meta.res, "case_alt")
  case.ref <- sum_cols_helper(meta.res, "case_ref")
  meta.res$case_freq <- case.alt / (case.alt + case.ref)
  control.alt <- sum_cols_helper(meta.res, "control_alt")
  control.ref <- sum_cols_helper(meta.res, "control_ref")
  meta.res$control_freq <- control.alt / (control.alt + control.ref)

  # Format output
  meta.res$exclude_cohorts[which(meta.res$exclude_cohorts == "")] <- NA
  colnames(meta.res)[which(colnames(meta.res)=="exclude_cohorts")] <- "cohorts_excluded_from_meta"
  cols.to.keep <- c(colnames(stats.merged)[1:keep.n.cols],
                    "n_nominal_cohorts", "top_cohort",
                    "cohorts_excluded_from_meta", "case_freq", "control_freq")
  return(as.data.frame(cbind(meta.res[, cols.to.keep],
                             meta.res[, grep("meta_", colnames(meta.res), fixed=T)])))
}


#' Load single cohort data for on-the-fly meta-analysis
#'
#' Load a list of genes impacted per CNV for a single cohort for on-the-fly meta-analyses
#'
#' @param cnvs.in path to input .tsv (see `details`)
#'
#'
#' @return data.frame with two columns:
#' * `$iscase` boolean for whether CNV was observed in case sample
#' * `$genes` list of genes impacted by this CNV
#'
#' @details `cnvs.in` must be a .tsv with at least the following two columns:
#' 1. `phenos` : semicolon-delimited list of all HPO terms corresponding to that CNV
#' 2. `genes` : semicolon-delimited list of genes impacted by that CNV
#' Other columns can be optionally present and will be ignored.
#'
#' @export
load.otf.cohort.cnvs <- function(cnvs.in){
  x <- read.table(cnvs.in, header=T, sep="\t", comment.char="")
  x <- x[which(x$ngenes>0), c("phenos", "genes")]
  x$iscase <- strsplit(x$phenos, split=";") != c("HEALTHY_CONTROL")
  x$phenos <- NULL
  x$genes <- strsplit(x$genes, split=";")
  return(x)
}


#' Load on-the-fly meta-analysis data
#'
#' Load all data required for on-the-fly meta-analysis
#'
#' @param meta.inputs.in path to input .tsv (see `details`)
#'
#' @return list with one element per row in `meta.inputs.in`.
#' Each cohort has four named features:
#' * `cnvs` : deletions and duplications as loaded by [load.otf.cohort.cnvs()]
#' * `n_case` : vector of number of cases per cohort
#' * `n_control` : vector of number of controls per cohort
#' * `cohorts` : vector of cohort names
#'
#' @details `meta.inputs.in` is expected to be a .tsv with the following five columns:
#' 1. `cohort` : name of cohort
#' 2. `n_case` : number of case samples
#' 3. `n_control` : number of control samples
#' 4. `DEL_path` : path to deletion BED file
#' 5. `DUP_path` : path to duplication BED file
#'
#' @seealso [load.otf.cohort.cnvs()]
#'
#' @export
load.otf.meta.dat <- function(meta.inputs.in){
  inputs <- read.table(meta.inputs.in, sep="\t", header=T)
  cohorts <- as.character(inputs$cohort)
  n_case <- as.numeric(inputs$n_case)
  n_control <- as.numeric(inputs$n_control)
  names(n_control) <- names(n_case) <- cohorts
  cnvs <- apply(inputs[, c("DEL_path", "DUP_path")], 1, function(paths){
    return(list("DEL" = load.otf.cohort.cnvs(paths[1]),
                "DUP" = load.otf.cohort.cnvs(paths[2])))
  })
  names(cnvs) <- cohorts
  return(list("cnvs" = cnvs, "n_case" = n_case,
              "n_control" = n_control, "cohorts" = cohorts))
}


#' On-the-fly meta-analysis of a gene list
#'
#' Compute on-the-fly meta-analysis of all CNVs that hit a list of genes
#'
#' @param meta-dat meta-analysis inputs as loaded by [load.otf.meta.dat()]
#' @param genes vector of genes of interest
#' @param cnv CNV type to evaluate (options: `DEL` or `DUP`)
#'
#' @seealso [load.otf.meta.dat()]
#'
#' @export
gene.meta.otf <- function(meta.dat, genes, cnv){
  stats.merged <- do.call("cbind", lapply(1:length(meta.dat$cnvs), function(i){
    cnvs <- meta.dat$cnvs[[i]][[cnv]]
    cohort <- meta.dat$cohorts[i]
    ncase.all <- meta.dat$n_case[i]
    nctrl.all <- meta.dat$n_control[i]
    hits <- which(sapply(cnvs$genes, function(glist){length(intersect(glist, genes)) > 0}))
    hits.iscase <- table(cnvs$iscase[hits])
    for(lab in c("TRUE", "FALSE")){
      if(!(lab %in% names(hits.iscase))){
        names <- c(names(hits.iscase), lab)
        hits.iscase <- c(hits.iscase, 0)
        names(hits.iscase) <- names
      }
    }
    ncase.alt <- hits.iscase["TRUE"]
    ncase.ref <- ncase.all - ncase.alt
    nctrl.alt <- hits.iscase["FALSE"]
    nctrl.ref <- nctrl.all - nctrl.alt
    outvals <- data.frame(ncase.alt, ncase.ref, nctrl.alt, nctrl.ref)
    colnames(outvals) <- paste(cohort, c("case_alt", "case_ref", "control_alt",
                                         "control_ref"), sep=".")
    return(outvals)
  }))
  stats.merged$exclude_cohorts <- ""
  return(meta.single(stats.merged, meta.dat$cohorts, row.idx=1))
}
