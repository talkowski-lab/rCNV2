#!/usr/bin/env Rscript

######################
#    rCNV Project    #
######################

# Copyright (c) 2021-Present Ryan L. Collins and the Talkowski Laboratory
# Distributed under terms of the MIT License (see LICENSE)
# Contact: Ryan L. Collins <rlcollins@g.harvard.edu>

# Functions used in association testing


#' Get sample counts from table
#'
#' Extract case/control sample sizes from a single cohort for association testing
#'
#' @param pheno.table.in path to phenotype table .tsv
#' @param cohort.name name of cohort
#' @param case.hpo HPO code for cases
#' @param control.hpo HPO code for controls
#'
#' @return list of two values:
#' 1. `$case.n` : number of cases
#' 2. `$control.n` : number of controls
#'
#' @export
get.sample.counts <- function(pheno.table.in, cohort.name, case.hpo, control.hpo){
  ptab <- read.table(pheno.table.in, header=T, sep="\t", comment.char="")
  case.n <- ptab[which(ptab[,1] == case.hpo),
                 which(colnames(ptab) == cohort.name)]
  control.n <- ptab[which(ptab[,1] == control.hpo),
                    which(colnames(ptab) == cohort.name)]
  return(list("case.n"=as.numeric(case.n),
              "control.n"=as.numeric(control.n)))
}


#' Load case/control CNV counts
#'
#' Loads a file of CNV counts in cases and controls per locus
#'
#' @param bed.in path to input BED3+ file
#' @param case.col.name name of column containing case CNV counts
#' @param case.n total number of case samples in cohort
#' @param control.col.name name of column containing control CNV counts
#' @param control.n total number of control samples in cohort
#' @param keep.n.cols number of columns from original BED format to retain
#'
#' @return data frame of CNV counts and frequencies per locus
#'
#' @details `bed.in` must adhere to format as generated by any of the following scripts:
#' * `analysis/sliding_windows/count_cnvs_per_window.py`
#' * `analysis/genes/count_cnvs_per_gene.py`
#' * `analysis/noncoding/count_cnvs_per_crb.py`
#'
#' @export
load.cc.cnv.counts <- function(bed.in, case.col.name, case.n, control.col.name,
                               control.n, keep.n.cols=3){
  bed <- read.table(bed.in, sep="\t", header=T, comment.char="")

  case.col.idx <- which(colnames(bed) == case.col.name)
  if(length(case.col.idx) == 0){
    stop(paste("--case-column \"",case.col.name,"\" cannot be found ",
               "in BED header", sep=""))
  }

  control.col.idx <- which(colnames(bed) == control.col.name)
  if(length(control.col.idx) == 0){
    stop(paste("--control-column \"",control.col.idx,"\" cannot be found ",
               "in BED header", sep=""))
  }

  bed <- bed[,c(1:keep.n.cols, case.col.idx, control.col.idx)]
  colnames(bed) <- c("chr", colnames(bed[2:keep.n.cols]), "case.CNV", "control.CNV")
  bed$case.ref <- case.n - bed$case.CNV
  bed$control.ref <- control.n - bed$control.CNV
  bed$case.CNV.freq <- bed$case.CNV / case.n
  bed$control.CNV.freq <- bed$control.CNV / control.n

  return(bed)
}


#' Single-locus Fisher's exact test
#'
#' Fisher's exact test for a single vector of CNV counts for one cohort and one locus
#'
#' @param counts vector of CNV counts (see `Details`)
#' @param alternative specify sidedness of test \[default: 'greater'\]
#'
#' @details `counts` must contain four values, in order:
#' 1. count of case CNV carriers
#' 2. count of control CNV carriers
#' 3. count of non-carrier cases
#' 4. count of non-carrier controls
#'
#'
#' @return named vector with association summary statistics, in the order of
#' P-value, odds ratio, and lower/upper bounds of 95% conf. int. of the OR
#'
#' @export
fisher.burden.test.single <- function(counts, alternative="greater"){
  case.cnv <- as.integer(counts[1])
  control.cnv <- as.integer(counts[2])
  case.ref <- as.integer(counts[3])
  control.ref <- as.integer(counts[4])

  if(case.cnv == 0 & control.cnv == 0){
    p <- 1
    or <- c(NA,NA,NA)
  }else{
    cnv.mat <- matrix(c(control.ref, case.ref, control.cnv, case.cnv),
                      byrow=T, nrow=2)

    p <- fisher.test(cnv.mat, alternative=alternative)$p.value
    or <- fisher.test(cnv.mat)
    or <- c(or$estimate, or$conf.int)
  }

  f.res <- as.numeric(c(p, or))
  names(f.res) <- c("p", "OR", "OR.lower", "OR.upper")
  return(f.res)
}


#' Compute Fisher's exact test lookup table
#'
#' Build a lookup table of all case/control count pairings for Fisher's Exact Tests
#'
#' @param bed data frame of case/control CNV counts as loaded by `load.cc.cnv.counts()`
#' @param keep.n.cols number of columns from original BED format to retain
#'
#' @return data frame of all unique case & control CNV counts
#'
#' @seealso [load.cc.cnv.counts()]
#'
#' @export
build.fisher.lookup.table <- function(bed, keep.n.cols=3){
  counts.df <- unique(bed[, (1:4)+keep.n.cols])
  counts.df <- counts.df[with(counts.df, order(control.CNV, case.CNV)),]

  f.stats <- t(apply(counts.df, 1, fisher.burden.test.single))

  f.table <- cbind(counts.df, f.stats)
  return(f.table)
}


#' Fisher's Exact Test for a list of loci
#'
#' Fisher's exact test of case:control CNV burden per locus for one or more loci
#'
#' @param bed data frame of case/control CNV counts as loaded by `load.cc.cnv.counts()`
#' @param keep.n.cols number of columns from original BED format to retain
#' @param precision maximum precision for floating point values
#'
#' @return data frame of association test resuts
#'
#' @seealso [fisher.burden.test.single()]
#'
#' @export
fisher.burden.test <- function(bed, keep.n.cols=3, precision=10){
  f.table <- build.fisher.lookup.table(bed, keep.n.cols)

  f.res <- merge(bed, f.table, sort=F, all.x=T, all.y=F)
  f.res <- f.res[with(f.res, order(chr, start)),]

  fisher.bed <- data.frame("chr" = f.res$chr,
                           "start" = f.res$start,
                           "end" = f.res$end,
                           "case_alt" = f.res$case.CNV,
                           "case_ref" = f.res$case.ref,
                           "case_freq" = round(f.res$case.CNV.freq, precision),
                           "control_alt" = f.res$control.CNV,
                           "control_ref" = f.res$control.ref,
                           "control_freq" = round(f.res$control.CNV.freq, precision),
                           "fisher_phred_p" = round(-log10(f.res$p), precision),
                           "fisher_OR" = round(f.res$OR, precision),
                           "fisher_OR_lower" = round(f.res$OR.lower, precision),
                           "fisher_OR_upper" = round(f.res$OR.upper, precision))

  return(merge(bed[, 1:keep.n.cols], fisher.bed, all.y=T, sort=F))
}


#' Calculate CNV odds ratio
#'
#' Compute basic OR of CNV carriers with adjustment for zero-inflation
#'
#' @param control_ref number of control non-carriers
#' @param control_alt number of control CNV carriers
#' @param case_ref number of case non-carriers
#' @param case_alt number of case CNV carriers
#' @param adj fixed value to use for continuity correction \[default: 0.5\]
#'
#' @details implements simple continuity correction with fixed value added to
#' all cells of contingency table
#'
#' @return odds ratio (numeric)
#'
#' @export
calc.or <- function(control_ref, control_alt, case_ref, case_alt, adj=0.5){
  case.odds <- (case_alt + adj) / (case_ref + adj)
  control.odds <- (control_alt + adj) / (control_ref + adj)
  case.odds / control.odds
}


#' Multi-cohort pairwise effect size comparison
#'
#' Make a grid of scatterplots of log odds ratios pairwise for all cohorts in a meta-analysis
#'
#' @param stats.list list of single-cohort association stats
#' @param pt.cex scaling factor for points
#'
#' @return None
#'
#' @seealso [read.assoc.stats.single()]
#'
#' @export or.corplot.grid
#' @export
or.corplot.grid <- function(stats.list, pt.cex=1){
  ncohorts <- length(stats.list)
  cohorts <- names(stats.list)
  par(mfrow=c(ncohorts, ncohorts))
  ymar <- 3
  xmar <- 3.7
  sapply(1:ncohorts, function(r){
    sapply(1:ncohorts, function(c){
      parmar <- c(ymar/2, xmar/2, ymar/2, xmar/2)
      # Set margins
      if(c==1){
        parmar[2] <- xmar-0.2; parmar[4] <- 0.2
      }
      if(c==ncohorts){
        parmar[2] <- 0.2; parmar[4] <- xmar-0.2
      }
      if(r==1){
        parmar[1] <- 0.2; parmar[3] <- ymar-0.2
      }
      if(r==ncohorts){
        parmar[1] <- ymar-0.2; parmar[3] <- 0.2
      }
      # Don't plot self-self correlations
      if(c==r){
        par(mar=parmar)
        plot(x=c(0, 1), y=c(0, 1), type="n",
             xaxt="n", xlab="", yaxt="n", ylab="")
        rect(xleft=par("usr")[1], xright=par("usr")[2],
             ybottom=par("usr")[3], ytop=par("usr")[4],
             bty="n", col="gray95")
        text(x=0.5, y=0.5, labels=bquote(italic(R)==1))
        # Otherwise, plot as normal
      }else{
        dens.scatter(x=log10(stats.list[[c]][, grep("odds_ratio", colnames(stats.list[[c]]), fixed=T)]),
                     y=log10(stats.list[[r]][, grep("odds_ratio", colnames(stats.list[[r]]), fixed=T)]),
                     parmar=parmar, pt.cex=pt.cex)
      }
      # Add headers & axes
      if(c==1){
        mtext(2, line=0.2, text=cohorts[r], font=2)
      }
      if(r==1){
        mtext(3, line=0.2, text=cohorts[c], font=2)
      }
      if(c==ncohorts){
        axis(4, labels=NA)
        axis(4, tick=F, line=-0.5, las=2)
        mtext(4, line=2.25, text=bquote(log[10](OR)), cex=0.8)
      }
      if(r==ncohorts){
        axis(1, labels=NA)
        axis(1, tick=F, line=-0.5)
        mtext(1, line=2, text=bquote(log[10](OR)) ,cex=0.8)
      }
    })
  })
}


#' Merge single-cohort association stats
#'
#' Merges a list of association statistics generated for multiple independent cohorts
#'
#' @param stats.list list of single-cohort association stats
#' @param cond.excl.in path to BED file of cohorts to be excluded on locus-specific basis
#' @param keep.n.cols number of columns from original BED format to retain
#'
#' @return data frame of association stats for all cohorts
#'
#' @seealso [read.assoc.stats.single()]
#'
#' @export
combine.single.cohort.assoc.stats <- function(stats.list, cond.excl.in=NULL,
                                              keep.n.cols=3){
  # Merge all cohorts
  merged <- stats.list[[1]]
  mergeby.cols <- colnames(merged)[1:keep.n.cols]
  for(i in 2:length(stats.list)){
    merged <- merge(merged, stats.list[[i]], by=mergeby.cols, all=F, sort=F)
  }
  merged[, -c(1:keep.n.cols)] <- apply(merged[, -c(1:keep.n.cols)], 2, as.numeric)

  # Count number of nominally significant individual cohorts
  n_nom_sig <- apply(merged[, grep(".p_value", colnames(merged), fixed=T)],
                     1, function(pvals){length(which(pvals<=0.05))})
  merged$n_nominal_cohorts <- n_nom_sig

  # Determine most significant cohort per row
  merged$top_cohort <- unlist(apply(merged[, grep(".p_value", colnames(merged), fixed=T)], 1,
                                    function(pvals){
                                      head(names(stats.list)[which(pvals == min(pvals, na.rm=T))], 1)
                                    }))
  merged <- merged[, -grep(".p_value", colnames(merged), fixed=T)]

  # Annotate rows with cohorts to be conditionally excluded
  if(!is.null(cond.excl.in)){
    cond.excl.df <- read.table(cond.excl.in, header=T, sep="\t", comment.char="")
    colnames(cond.excl.df)[1] <- gsub('^X.', '', colnames(cond.excl.df)[1])
    cond.excl.df$exclude_cohorts
    merged <- merge(merged, cond.excl.df, by=mergeby.cols,
                    all.x=T, all.y=F, sort=F)
  }else{
    merged$exclude_cohorts <- ""
  }

  return(as.data.frame(merged))
}


#' Empirical continuity correction for meta-analyses
#'
#' Apply empirical continuity correction to meta-analysis data frame per Sweeting
#' et al., Stat. Med., 2004 (section 3.3)
#'
#' @param meta.df meta-analysis data frame as generated by [make.meta.df()]
#' @param cc.sum scaling factor for total continuity correction to be distributed
#' among cases and controls
#'
#' @return data frame of meta-analysis stats after applying continuity correction
#'
#' @seealso [make.meta.df()]
#'
#' @export
sweeting.correction <- function(meta.df, cc.sum=0.01){
  # Count number of carriers & non-carriers
  n.alt <- sum(meta.df[, grep("_alt", colnames(meta.df), fixed=T)])
  n.ref <- sum(meta.df[, grep("_ref", colnames(meta.df), fixed=T)])
  # Require at least one CNV to be observed
  if(n.alt>0){
    nt <- n.alt
    R <- n.ref/n.alt
    # Pooled odds ratio estimate of all non-zero studies with at least one case sample
    nonzero.studies <- intersect(which(apply(meta.df[, grep("_alt", colnames(meta.df), fixed=T)], 1, sum)>0),
                                 which(apply(meta.df[, grep("case_", colnames(meta.df), fixed=T)], 1, sum)>0))
    nonzero.case.odds <- sum(meta.df$case_alt[nonzero.studies])/sum(meta.df$case_ref[nonzero.studies])
    nonzero.control.odds <- sum(meta.df$control_alt[nonzero.studies])/sum(meta.df$control_ref[nonzero.studies])
    if(!is.nan(nonzero.case.odds) & !is.nan(nonzero.control.odds)){
      if(nonzero.control.odds>0){
        ohat <- nonzero.case.odds/nonzero.control.odds
        # Otherwise, apply standard continuity correction of 0.5 to pooled estimate if no CNVs observed in controls
      }else{
        nonzero.case.odds <- (sum(meta.df$case_alt[nonzero.studies])+0.5)/(sum(meta.df$case_ref[nonzero.studies])+0.5)
        nonzero.control.odds <- (sum(meta.df$control_alt[nonzero.studies])+0.5)/(sum(meta.df$control_ref[nonzero.studies])+0.5)
        ohat <- nonzero.case.odds/nonzero.control.odds
      }
      # Otherwise, apply standard continuity correction to pooled estimate of *all* studies
    }else{
      nonzero.case.odds <- (sum(meta.df$case_alt)+0.5)/(sum(meta.df$case_ref)+0.5)
      nonzero.control.odds <- (sum(meta.df$control_alt)+0.5)/(sum(meta.df$control_ref)+0.5)
      ohat <- nonzero.case.odds/nonzero.control.odds
    }

    # Solve for kc & kt
    kc <- R/(R+ohat)
    kt <- ohat/(R+ohat)
    # Compute continuity corrections
    cor.case_alt <- cc.sum * kt
    cor.case_ref <- cc.sum * kc
    cor.control_alt <- cc.sum * (nt + kt)
    cor.control_ref <- cc.sum * ((nt*R) + kc)
    # Apply continuity corrections
    meta.df$case_alt <- meta.df$case_alt + cor.case_alt
    meta.df$case_ref <- meta.df$case_ref + cor.case_ref
    meta.df$control_alt <- meta.df$control_alt + cor.control_alt
    meta.df$control_ref <- meta.df$control_ref + cor.control_ref
  }
  return(meta.df)
}


#' Create meta-analysis data frame
#'
#' Make meta-analysis data frame for CNV association testing of a single locus
#'
#' @param stats.merged association statistics for all cohorts (see [combine.single.cohort.assoc.stats()])
#' @param cohorts vector of all cohort names
#' @param row.idx row index in `stats.merged` corresponding to locus of interest
#' @param empirical.continuity boolean indicator to apply Sweeting empirical
#' continuity correction \[default: TRUE\]
#'
#' @return data frame of statistics ready for meta-analysis
#'
#' @seealso [combine.single.cohort.assoc.stats()]
#'
#' @export
make.meta.df <- function(stats.merged, cohorts, row.idx, empirical.continuity=T){
  ncohorts <- length(cohorts)
  meta.df <- data.frame("cohort"=1:ncohorts,
                        "control_ref"=as.numeric(stats.merged[row.idx, grep("control_ref", colnames(stats.merged), fixed=T)]),
                        "case_ref"=as.numeric(stats.merged[row.idx, grep("case_ref", colnames(stats.merged), fixed=T)]),
                        "control_alt"=as.numeric(stats.merged[row.idx, grep("control_alt", colnames(stats.merged), fixed=T)]),
                        "case_alt"=as.numeric(stats.merged[row.idx, grep("case_alt", colnames(stats.merged), fixed=T)]),
                        "cohort_name"=cohorts)
  if(empirical.continuity==T){
    meta.df <- sweeting.correction(meta.df)
  }
  return(meta.df)
}


#' Single locus meta-analysis
#'
#' Perform a meta-analysis of CNVs for a single locus
#'
#' @param stats.merged association statistics for all cohorts (see [combine.single.cohort.assoc.stats()])
#' @param cohorts vector of all cohort names
#' @param row.idx row index in `stats.merged` corresponding to locus of interest
#' @param model specify meta-analysis model to use (see `Details`)
#' @param empirical.continuity boolean indicator to apply Sweeting empirical
#' continuity correction \[default: TRUE\]
#' @param drop_top_cohort boolean indicator to drop most significant individual
#' cohort from meta-analysis \[default: FALSE\]
#'
#' @details `model` accepts several character inputs, including:
#' * `fe` : fixed-effects model implemented by [metafor::rma.uni()] \[default\]
#' * `re` : random-effects model implemented by [metafor::rma.uni()]
#' * `mh` : Mantel-Haenszel model implemented by [metafor::rma.mh()]
#'
#' @return numeric vector of meta-analysis summary statistics
#'
#' @seealso [make.meta.df()]
#'
#' @export
meta.single <- function(stats.merged, cohorts, row.idx, model="fe",
                        empirical.continuity=T, drop_top_cohort=F){
  # Collect list of cohorts to exclude
  exclude.cohorts <- unlist(strsplit(stats.merged$exclude_cohorts[row.idx], split=";"))
  if(drop_top_cohort == TRUE){
    exclude.cohorts <- unique(sort(c(exclude.cohorts, stats.merged$top_cohort[row.idx])))
  }
  exclude.cohorts <- exclude.cohorts[which(exclude.cohorts %in% cohorts)]
  cohorts <- setdiff(cohorts, exclude.cohorts)

  # Hard drop cohorts for exclusion from stats.merged for this row
  if(length(exclude.cohorts) > 0){
    cols.to.drop <- unique(sort(as.vector(sapply(exclude.cohorts, function(cohort){
      grep(paste(cohort, ".", sep=""), colnames(stats.merged))
    }))))
    if(length(cols.to.drop) > 0){
      stats.merged.sub <- stats.merged[row.idx, -cols.to.drop]
    }
  }else{
    stats.merged.sub <- stats.merged[row.idx, ]
  }

  # Check that at least two cohorts remain
  n.cohorts <- length(cohorts)
  if(n.cohorts >= 2){

    # Check if at least one CNV was observed
    n.cnvs <- sum(stats.merged.sub[, grep("_alt", colnames(stats.merged.sub), fixed=T)])
    if(n.cnvs > 0){
      meta.df <- make.meta.df(stats.merged.sub, cohorts, 1, empirical.continuity)
      # If strictly zero case CNVs are observed, unable to estimate effect size
      if(all(meta.df$case_alt==0)){
        out.v <- c(rep(NA, 4), 0)
      }else{
        # Meta-analysis
        if(model=="re"){
          meta.res <- tryCatch(rma.uni(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt,
                                       measure="OR", data=meta.df, method="REML", random = ~ 1 | cohort, slab=cohort_name,
                                       add=0, drop00=F, correct=F, digits=5, control=list(maxiter=100, stepadj=0.5)),
                               error=function(e){
                                 print(paste("row", row.idx, "failed to converge. Retrying with more iterations...", sep=" "))
                                 rma.uni(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt,
                                         measure="OR", data=meta.df, method="REML", random = ~ 1 | cohort, slab=cohort_name,
                                         add=0, drop00=F, correct=F, digits=5, control=list(maxiter=10000, stepadj=0.4))
                               })
          out.v <- as.numeric(c(meta.res$b[1,1], meta.res$ci.lb, meta.res$ci.ub,
                                meta.res$zval, -log10(meta.res$pval)))
        }else if(model=="mh"){
          meta.res <- rma.mh(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt,
                             measure="OR", data=meta.df, slab=cohort_name,
                             add=0, drop00=F, correct=F)
          out.v <- as.numeric(c(meta.res$b, meta.res$ci.lb, meta.res$ci.ub,
                                meta.res$zval, -log10(meta.res$MHp)))
        }else if(model=="fe"){
          meta.res <- tryCatch(rma.uni(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt,
                                       measure="OR", data=meta.df, method="FE", slab=cohort_name,
                                       add=0, drop00=F, correct=F, digits=5, control=list(maxiter=100, stepadj=0.5)),
                               error=function(e){
                                 print(paste("row", row.idx, "failed to converge. Retrying with more iterations...", sep=" "))
                                 rma.uni(ai=control_ref, bi=case_ref, ci=control_alt, di=case_alt,
                                         measure="OR", data=meta.df, method="FE", slab=cohort_name,
                                         add=0, drop00=F, correct=F, digits=5, control=list(maxiter=10000, stepadj=0.4))
                               })
          out.v <- as.numeric(c(meta.res$b[1,1], meta.res$ci.lb, meta.res$ci.ub,
                                meta.res$zval, -log10(meta.res$pval)))
        }
        # Force to p-values reflecting Ha : OR > 1
        if(!is.na(out.v[1]) & !is.na(out.v[5])){
          if(out.v[1] < 0){
            out.v[5] <- 0
          }
        }
      }
      return(out.v)
    }else{
      return(rep(NA, 5))
    }
  }else{
    return(rep(NA, 5))
  }
}


#' Create meta-analysis lookup table
#'
#' Make meta-analysis lookup table to shorten time required to run full meta-analysis
#'
#' @param stats.merged association statistics for all cohorts (see [combine.single.cohort.assoc.stats()])
#' @param cohorts vector of all cohort names
#' @param model specify meta-analysis model to use (see `Details`)
#' @param empirical.continuity boolean indicator to apply Sweeting empirical
#' continuity correction \[default: TRUE\]
#'
#' @details `model` accepts several character inputs, including:
#' * `fe` : fixed-effects model implemented by [metafor::rma.uni()] \[default\]
#' * `re` : random-effects model implemented by [metafor::rma.uni()]
#' * `mh` : Mantel-Haenszel model implemented by [metafor::rma.mh()]
#'
#' @return data frame of all unique case & control CNV counts
#'
#' @export
make.meta.lookup.table <- function(stats.merged, cohorts, model, empirical.continuity=T){
  unique.counts.df <- unique(stats.merged[, sort(unique(c(grep("_ref", colnames(stats.merged), fixed=T),
                                                          grep("_alt", colnames(stats.merged), fixed=T),
                                                          which(colnames(stats.merged) == "exclude_cohorts"))))])

  unique.stats <- t(sapply(1:nrow(unique.counts.df), function(i){
    meta.single(unique.counts.df, cohorts, i, model, empirical.continuity)
  }))

  lookup.table <- cbind(unique.counts.df, unique.stats)
  stat.colnames <- c("meta_lnOR", "meta_lnOR_lower", "meta_lnOR_upper", "meta_z", "meta_phred_p")
  colnames(lookup.table)[(ncol(lookup.table)-4):ncol(lookup.table)] <- stat.colnames

  return(lookup.table)
}


#' Z-score saddlepoint approximation
#'
#' Apply saddlepoint approximation to vector of Z-scores to generate adjusted P-values
#'
#' @param zscores numeric vector of Z-scores
#' @param phred boolean indicator of whether to -log10-scale adjusted P-values
#'
#' @return numeric vector of adjusted P-values
#'
#' @export
saddlepoint.adj <- function(zscores, phred=T){
  mu.hat <- mean(zscores, na.rm=T)
  sd.hat <- sd(zscores, na.rm=T)
  cumuls <- gaussianCumulants(mu.hat, sd.hat)
  dx <- 0.01
  x <- seq(-40, 40, dx)
  saddle.pdf <- saddlepoint(x, 1, cumuls)$approx
  saddle.cdf <- cumsum(saddle.pdf * 0.01)
  calc.saddle.p <- function(z){if(!is.na(z)){1 - tail(saddle.cdf[which(x<z)], 1)}else{NA}}
  new.pvals <- sapply(zscores, calc.saddle.p)
  if(phred==T){
    return(-log10(new.pvals))
  }else{
    return(new.pvals)
  }
}


#' CNV meta-analysis for one or more loci
#'
#' Conduct CNV association meta-analyses for one or more cohorts at one or more loci
#'
#' @param stats.merged association statistics for all cohorts (see [combine.single.cohort.assoc.stats()])
#' @param cohorts vector of all cohort names
#' @param model specify meta-analysis model to use (see `Details`)
#' @param saddle boolean indicator of whether to apply saddlepoint approximation
#' (see [saddlepoint.adj()]) \[default: TRUE\]
#' @param secondary boolean indicator to also compute meta-analysis statistics
#' after dropping most significant individual cohort \[default: TRUE\]
#' @param keep.n.cols number of columns from original BED format to retain
#'
#' @details `model` accepts several character inputs, including:
#' * `fe` : fixed-effects model implemented by [metafor::rma.uni()] \[default\]
#' * `re` : random-effects model implemented by [metafor::rma.uni()]
#' * `mh` : Mantel-Haenszel model implemented by [metafor::rma.mh()]
#'
#' @return data frame of meta-analysis summary statistics
#'
#' @export
meta <- function(stats.merged, cohorts, model="fe", saddle=T, secondary=T, keep.n.cols=3){
  # Make meta-analysis lookup table
  meta.lookup.table <- make.meta.lookup.table(stats.merged, cohorts, model,
                                              empirical.continuity=T)

  # Merge stats into full list
  meta.res <- merge(stats.merged, meta.lookup.table, sort=F, all.x=T, all.y=F)
  meta.res <- meta.res[with(meta.res, order(chr, start)), ]

  # Adjust P-values using saddlepoint approximation of null distribution, if optioned
  if(saddle==T){
    meta.res$meta_phred_p <- saddlepoint.adj(meta.res$meta_z)
  }

  # Compute secondary P-value
  if(secondary==T){
    meta.res.secondary <- as.data.frame(t(sapply(1:nrow(meta.res), function(i){
      meta.single(meta.res, cohorts, i, model, empirical.continuity=T, drop_top_cohort=T)
    })))
    colnames(meta.res.secondary) <- c("meta_lnOR", "meta_lnOR_lower", "meta_lnOR_upper", "meta_z", "meta_phred_p")

    # Saddlepoint on secondary, if optioned
    if(saddle==T){
      meta.res.secondary$meta_phred_p <- saddlepoint.adj(meta.res.secondary$meta_z)
    }

    meta.res$meta_lnOR_secondary <- meta.res.secondary$meta_lnOR
    meta.res$meta_lnOR_lower_secondary <- meta.res.secondary$meta_lnOR_lower
    meta.res$meta_lnOR_upper_secondary <- meta.res.secondary$meta_lnOR_upper
    meta.res$meta_z_secondary <- meta.res.secondary$meta_z
    meta.res$meta_phred_p_secondary <- meta.res.secondary$meta_phred_p
  }

  # Compute pooled carrier frequencies (while taking into account excluded cohorts)
  sum_cols_helper <- function(meta.res, substring){
    hits <- grep(substring, colnames(meta.res), fixed=T)
    excl.cohorts <- strsplit(meta.res$exclude_cohorts, split=";")
    sapply(1:nrow(meta.res), function(i){
      excl <- unique(unlist(sapply(excl.cohorts[[i]], grep, x=colnames(meta.res), fixed=TRUE)))
      keep <- setdiff(hits, excl)
      sum(as.numeric(meta.res[i, keep]), na.rm=T)
    })
  }
  case.alt <- sum_cols_helper(meta.res, "case_alt")
  case.ref <- sum_cols_helper(meta.res, "case_ref")
  meta.res$case_freq <- case.alt / (case.alt + case.ref)
  control.alt <- sum_cols_helper(meta.res, "control_alt")
  control.ref <- sum_cols_helper(meta.res, "control_ref")
  meta.res$control_freq <- control.alt / (control.alt + control.ref)

  # Format output
  meta.res$exclude_cohorts[which(meta.res$exclude_cohorts == "")] <- NA
  colnames(meta.res)[which(colnames(meta.res)=="exclude_cohorts")] <- "cohorts_excluded_from_meta"
  cols.to.keep <- c(colnames(stats.merged)[1:keep.n.cols],
                    "n_nominal_cohorts", "top_cohort",
                    "cohorts_excluded_from_meta", "case_freq", "control_freq")
  return(as.data.frame(cbind(meta.res[, cols.to.keep],
                             meta.res[, grep("meta_", colnames(meta.res), fixed=T)])))
}

